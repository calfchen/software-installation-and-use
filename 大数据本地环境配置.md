# 大数据本地环境配置

主要参考以下两篇博客:

[Ubuntu下spark开发（Local模式）](https://blog.csdn.net/anita0221/article/details/60570041)

[在Ubuntu16.04中配置Anaconda（Python2.7）以支持Spark2.0（Pyspark）](https://blog.csdn.net/duxu24/article/details/53587451)

[Ubuntu spark 搭建_在Ubuntu14.04 64bit上搭建单机Spark环境](http://www.linuxdown.net/install/soft/2016/0102/4281.html)


[在Ubuntu14.04 64bit上搭建单机Spark环境，IDE为Intelli IDEA](http://www.cnblogs.com/CherishFX/p/5302376.html)

## 1 安装 jdk8

在 [oracle官网](http://www.oracle.com/technetwork/java/javase/downloads/index.html)下载jdk8,下载别的版本,请自及查询版本依赖.

解压文件:

    sudo tar -zxvf jdk-8u171-linux-x64.tar.gz -C /opt/jdk

添加配置文件:

    sudo vi /etc/profile

加入以下配置：

    #Set JDK JDK环境变量 
    export JAVA_HOME=/opt/jdk/jdk1.8.0_171
    export JRE_HOME=${JAVA_HOME}/jre 
    export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib 
    export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:$PATH


重新载入 profile 文件:

    source /etc/profile

验证 java 是否安装成功:

    java, javac, java -version

## 2 安装 scala

在[scala官网](http://www.scala-lang.org/)下载scala.

解压文件:

    sudo tar -zxvf scala-2.12.6.tgz -C /opt/scala

添加配置文件/etc/profile:

    #set scala path 
    export SCALA_HOME=/opt/scala/scala-2.12.6
    export PATH=${SCALA_HOME}/bin:$PATH

重新载入 profile 文件:

    source /etc/profile

验证 scala 是否安装成功:

    scala -version

## 3 安装 spark

在[spark官网](http://spark.apache.org/downloads.html)下载scala

解压文件:

    sudo tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C /opt/spark
    
添加配置文件/etc/profile:

    #set spark path
    export SPARK_HOME=/opt/spark/spark-2.3.0-bin-hadoop2.7
    export PATH=${SPARK_HOME}/bin:$PATH

重新载入 profile 文件:

    source /etc/profile

验证 spark 是否安装成功:

    spark-shell

## 4 配置 pyspark 默认使用 spyder,或者 jupyter-notebook

    $ sudo vi /etc/profile

在末尾加入：

    export ANACONDA_ROOT=~/anaconda3
    export PYSPARK_DRIVER_PYTHON=$ANACONDA_ROOT/bin/ipython notebook
    export PYSPARK_PYTHON=$ANACONDA_ROOT/bin/python
    export PYSPARK_DRIVER_PYTHON_OPTS="notebook"

使用spyder,修改第二行:

    export PYSPARK_DRIVER_PYTHON=$ANACONDA_ROOT/bin/spyder
